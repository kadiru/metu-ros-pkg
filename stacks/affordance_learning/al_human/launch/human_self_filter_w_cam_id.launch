<launch>

  <arg name="camera" default="left_camera"/>

  <!--include file="$(find openni_launch)/launch/openni.launch"/-->

  <param name="human_description" command="$(find xacro)/xacro.py '$(find al_human)/defs/human_w_hat.urdf.xacro'" />

  <!--node pkg="al_human" type="human_tf_publisher" name="human_tf_publisher"/-->

  <!--node name="robot_state_publisher" pkg="robot_state_publisher" type="state_publisher">
	<remap from="joint_states" to="human_joint_states" />
  </node-->

  <node pkg="robot_self_filter" type="self_filter" respawn="true" output="screen" name="human_self_filter">
  <!--remap from="cloud_in" to="camera/depth/points" /-->
  <remap from="robot_description" to="human_description"/>
  <remap from="cloud_in" to="$(arg  camera)/depth/points"/>
  <remap from="cloud_out" to="cloud/human_self_fltrd"/>

  <!-- The frame of the sensor used to obtain the data to be
       filtered; This parameter is optional. If it is not specified,
       shadow points will be considered outside -->
     <!--param name="sensor_frame" type="string" value="/openni_rgb_optical_frame" /-->
     <param name="sensor_frame" type="string" value="$(arg camera)_rgb_optical_frame" />

     <!-- Minimum distance to sensor (for point not to be considered inside) -->
     <param name="min_sensor_dist" type="double" value="0.01" />

     <!-- The padding to be added for the body parts the robot can see -->
     <param name="self_see_padd" type="double" value="0.02" />

     <!-- The scaling to be added for the body parts the robot can see -->
     <param name="self_see_scale" type="double" value="1.0" />
    
     <!-- The names of the links the sensor can see -->
	<rosparam file="$(find al_human)/config/human_self_filter.yaml" command="load" />
	<param name="subsample_value" type="double" value="0.0"/>
  </node>
</launch>
